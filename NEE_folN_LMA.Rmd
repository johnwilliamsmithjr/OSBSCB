---
title: "NEE, LMA, and Foliar N"
author: "John W Smith"
date: "2/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Goals

In this document, we hope to extract three components for our analysis of terrestrial carbon at OSBS. Those are: NEE (flux), LMA (driver), and Foliar N (driver). 

## LMA

LMA stands for "Leaf Mass per Area". In order to extract this, we will use information from the TRY database (Kattege et al, 2011; https://onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2486.2011.02451.x). 

The TRY database does not directly have LMA, but it does have Specific Leaf Area (SLA). In order to obtain the LMA, we will do some unit conversion from Table 5.

LMA has the units $\frac{gC}{m^2 LA}$, while SLA has the units $\frac{mm^2 LA}{mgC}$. Then, $\frac{1}{SLA}$ has the units $\frac{mgC}{mm^2 LA}$. We can do a simple unit conversion using the SLA for "Tree Needleleaved Evergreen" in Table 5, to obtain:

$\frac{1}{SLA} \frac{mgC}{mm^2 LA} \cdot \frac{1g}{1000mg} \frac{1000mm}{1m} \frac{1000mm}{1m} = 200 \frac{gC}{m^2 LA}$.

Thus our LMA for OSBS is $LMA = 200$. 

## Foliar N

To find the value of Foliar N we once again use the information from the TRY database. The value $N_a$ in Table 5 represents the leaf nitrogen content per area $\frac{g}{m^2}$. These are the exact units we are looking for in Foliar N as well - thus we can simply look at the value in Table 5 for "Tree Needleleaved Evergreen", which gives us a value of $N_{fol} = 2.62$. 

## NEE

The last of the information that we are trying to extract is NEE (Net Ecosystem Exchange), which comes to us as "NSAE" (Net Surface Atmosphere Exchange) from the NEON data. 

We have some good starting points from the neon4cast resources, in particular the script here: https://github.com/eco4cast/neon4cast-terrestrial/blob/master/02_terrestrial_targets.R 

We can modify this script to work for aggregating our NEE data for OSBS. In particular we will add a variable called "threshold" that will govern how many 30 min samples must have passed QAQC in the 24 hour period that we are aggregating over. 

```{r}
## load necessary libraries
library(neonUtilities)
library(neonstore)
library(tidyverse)
library(lubridate)
#library(contentid)
```

```{r}
## get NSAE data from neon
neon_download(product = 'DP4.00200.001', site = 'OSBS')

```

```{r}
## read in NSAE data using neon_read
flux_data <- neon_read("nsae-basic")
```

```{r}
## do some filtering of the flux data
co2_data <- flux_data %>% 
  mutate(time = as_datetime(timeBgn)) %>%  
  #filter(qfqm.fluxCo2.turb.qfFinl == 0 & data.fluxCo2.turb.flux > -50 & data.fluxCo2.turb.flux < 50 & data.fluxMome.turb.veloFric >= 0.1) %>% 
  filter(data.fluxCo2.turb.flux > -50 & 
           data.fluxCo2.turb.flux < 50 & 
           data.fluxMome.turb.veloFric >= 0.1,
         year(time) >= 2019) %>% 
  select(time,data.fluxCo2.turb.flux, siteID) %>% 
  rename(nee = data.fluxCo2.turb.flux)
## plot filtered data
ggplot(co2_data, aes(x = time, y = nee)) +
  geom_point() +
  facet_wrap(~siteID)

  
```

```{r}
## aggregation
earliest <- min(as_datetime(c(co2_data$time)), na.rm = TRUE)
latest <- max(as_datetime(c(co2_data$time)), na.rm = TRUE)


full_time <- seq(min(c(co2_data$time), na.rm = TRUE), 
                 max(c(co2_data$time), na.rm = TRUE), 
                 by = "30 min")

full_time <- tibble(time = rep(full_time, 1),
                    siteID = c(rep("OSBS", length(full_time))))
                               
flux_target_30m <- left_join(full_time, co2_data, by = c("time", "siteID"))

threshold <- 40
valid_dates_nee <- flux_target_30m %>% 
  mutate(date = as_date(time)) %>% 
  filter(!is.na(nee)) %>% # & !is.na(le)) %>% 
  group_by(date, siteID) %>% 
  summarise(count = n()) %>% 
  filter(count >= threshold)

flux_target_daily <- flux_target_30m %>% 
  mutate(date = as_date(time)) %>% 
  group_by(date, siteID) %>% 
  summarize(nee = mean(nee, na.rm = TRUE)) %>% 
  mutate(nee = ifelse(date %in% valid_dates_nee$date, nee, NA),
         nee = ifelse(is.nan(nee), NA, nee)) %>% 
  rename(time = date) %>% 
  mutate(nee = (nee * 12 / 1000000) * (60 * 60 * 24))
```

```{r}
num_points <- flux_target_daily %>% 
  na.omit() %>% 
  nrow()

total_points <- flux_target_daily %>% 
  nrow()

num_points/total_points


```

```{r}
## plot aggregated data
ggplot(flux_target_daily, aes(x = time, y = nee)) + 
  geom_point() +
  geom_smooth()
```

